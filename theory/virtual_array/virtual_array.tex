\documentclass[14pt]{extarticle} 
\usepackage{authblk, helvet, amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry, hyperref, lscape, makecell, longtable}
\usepackage[binary-units=true]{siunitx}
\renewcommand{\familydefault}{\sfdefault}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}


\title{Virtual microphone array: theoretical aspects}
\author{Iran R. Roman \thanks{roman@nyu.edu}}
\affil{Music and Audio Research Laboratory, New York University}
\date{}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}

\section{Theoretical framework}

\begin{itemize}

\item Consider an audio signal $a(t)$ and an environment $h$ with LTI impulse response $h_c(t)$, where $c$ denotes the cartesian coordinates $c=(x,y,z)$ where the impulse respose $h_c(t)$ exists.

\item $b_c(t) = (a(t) \ast h_c(t))$ is the convolution between the audio signal and the environment's impulse response at location $c=(x,y,z)$. 

\item Also consider a microphone $u$ with impulse response $u(t)$. Measuring signal $a(t)$ at location $c$ in the environment $h$ using microphone $u$ will result in recording $r_c(t) = (u(t) \ast b_c(t))$.

\item Given $r_c(t)$, one can obtain the time-delayed version $r_c(t-T)$ via convolution with a dirac delta function $\delta(T)$. So $r_c(t-T) = (r_c(t) \ast \delta(T))$. The only challenge will be the tail or head of t

\item The virtual array will consist of $N$ identical microphones $u$ (or $N$ microphones with different impulse responses) physically distributed throughout the environment.

\item All microphones in the virtual array listen a common signal $a(t)$, but at different locations. The measurement of the signal $a(t)$ in room $h$ by microphone $u_n$ at location $c$ is given by $r_{(n,c_n)}(t) = (u_n(t) \ast b_{c_n}(t)) = \big(u_n(t) \ast (a(t) \ast h_{c_n}(t))\big)$.

\item Microphones in the virtual array may differ in position $c$ and impulse response $u(t)$. The different locations will change the room impulse response $h_c(t)$ that each microphone reads. Moreover, since the signal $a(t)$ travels at the speed of sound, the version of $a(t)$ reaching a specific microphone will be delayed with respect to other microphones. However, as we already discussed, delays can be easly accounted for with delta functions.  

\item Because the microphones in the virtual array share a common signal $a(t)$, there exists an idealized transfer function $f(x)$ that can convert $r_{(n,c_n)}(t)$ into $r_{(m,c_m)}(t) = r_{(n,c_n)}(t) \ast f(r_{(n,c_n)}(t),c_m)$. 

\item Such idealized transfer function does not exist in practice. However, a transfer function $l(x)$ can convert $r_{(n,c_n)}(t)$ into $r_{(m,c_m)}(t)$ with error $r_{(m,c_m)}(t) = r_{(n,c_n)}(t) \ast l(r_{(n,c_n)}(t)) + \epsilon$.

\item The question here is, how do we approximate $l(x)$ ?

\item Because of relative location, microphone $u_m$ will receive the signal after (or before) microphone $u_n$. Forecasting (or inverse forecasting) at the head or tail of $r_{(c,c_n)}(t)$ will be the first step in the signal processing pipeline. 

\item Assuming a virtual array with a volume in the order of few meters, the delay between microphones will be in the order of few milliseconds (speed of sound in air is around \SI{343}{\metre\per\second}).

\item This delay of a few milliseconds could be accounted for by phase-shifts implicit in filtering. The only exception would be sharply transient events that could be completely missing in a recording, specially at the tail of the signal, or at the head in anechoicc onditions.

\item Existing approaches for audio signal forecasting include dilated convolutions \cite{oord2016wavenet} and transformers \cite{child2019generating}. 

\item A CNN can learn embeddings from $r_{(n,c_n)}(t)$ time-frequency features. 2D kernels will allow for learning of sound event detection and room acoustics. 

\item Signal processing pipeline to generate $r_{(m,c_m)}(t)$ given $r_{(n,c_n)}(t)$
	\begin{enumerate}

	\item Forecasting (or its inverse) of $r_{(n,c_n)}(t)$, given the relative location between microphones $u_n$ and $u_m$ (in the order of milliseconds).

	\item Feature extraction of $r_{(n,c_n)}(t)$ with STFT.

	\item Neural embedding learning from $r_{(n,c_n)}$ features using a CNN. By the last CNN layer, the time axis has been eliminated.

	\item The embedding output and spatial information delta $c_n  - c_m$ are passed to dense layers that produce the parameters for a cascade of biquads that approximate $h(t)$.

	\end{enumerate}

\item Signal processing pipeline to generate $r_{(m,c_m)}(t-T)$ given $r_{(n,c_n)}(t)$

\item Signal processing pipeline to generate $r_{(n,c_n)}(t-T)$ given $r_{(n,c_n)}(t)$

\end{itemize}


\bibliography{references}
\bibliographystyle{plain}

\end{document}
